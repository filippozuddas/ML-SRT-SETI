{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filippozuddas/ML-SRT-SETI/blob/main/notebooks/train_CVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "TPoHtJa2I_E4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TPoHtJa2I_E4",
        "outputId": "22c096c8-c7a3-4812-ec07-a68be7dfc702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting setigen\n",
            "  Downloading setigen-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from setigen) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from setigen) (1.15.3)\n",
            "Requirement already satisfied: astropy>=5.0.4 in /usr/local/lib/python3.11/dist-packages (from setigen) (7.1.0)\n",
            "Collecting blimpy>=2.1.4 (from setigen)\n",
            "  Downloading blimpy-2.1.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from setigen) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.11/dist-packages (from setigen) (4.67.1)\n",
            "Collecting sphinx-rtd-theme>=0.4.3 (from setigen)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting pytest-cov>=4.1.0 (from setigen)\n",
            "  Downloading pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.4.28.0.37.27 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (0.2025.6.23.0.39.50)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (6.0.2)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (24.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (75.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (3.14.0)\n",
            "Collecting hdf5plugin (from blimpy>=2.1.4->setigen)\n",
            "  Downloading hdf5plugin-6.0.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (5.9.5)\n",
            "Collecting pyparsing==2.4.7 (from blimpy>=2.1.4->setigen)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (1.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (2.9.0.post0)\n",
            "Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov>=4.1.0->setigen)\n",
            "  Downloading coverage-7.11.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: pluggy>=1.2 in /usr/local/lib/python3.11/dist-packages (from pytest-cov>=4.1.0->setigen) (1.6.0)\n",
            "Requirement already satisfied: pytest>=7 in /usr/local/lib/python3.11/dist-packages (from pytest-cov>=4.1.0->setigen) (8.3.5)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme>=0.4.3->setigen) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme>=0.4.3->setigen) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme>=0.4.3->setigen)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=7->pytest-cov>=4.1.0->setigen) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->blimpy>=2.1.4->setigen) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->blimpy>=2.1.4->setigen) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2025.6.15)\n",
            "Downloading setigen-2.7.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blimpy-2.1.4-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_cov-7.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coverage-7.11.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.3/249.3 kB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdf5plugin-6.0.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (44.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyparsing, coverage, hdf5plugin, sphinxcontrib-jquery, pytest-cov, blimpy, sphinx-rtd-theme, setigen\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydot 3.0.4 requires pyparsing>=3.0.9, but you have pyparsing 2.4.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blimpy-2.1.4 coverage-7.11.3 hdf5plugin-6.0.0 pyparsing-2.4.7 pytest-cov-7.0.0 setigen-2.7.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "14f022560c0f40b39d44ca6f845cc958",
              "pip_warning": {
                "packages": [
                  "pyparsing",
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install setigen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "vVtKbNZ0Jc_9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVtKbNZ0Jc_9",
        "outputId": "11117107-0efa-4c86-c72d-73af2af94505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Setup completed!\n",
            "   Working directory: /home/filippo/TirocinioSETI/ML-SRT-SETI\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# --- CONFIGURAZIONE PERCORSI ---\n",
        "PROJECT_ROOT = \"/content/filippo/ML-SRT-SETI\"\n",
        "PROJECT_ROOT_LOCAL = \"/home/filippo/TirocinioSETI/ML-SRT-SETI\"\n",
        "\n",
        "if not os.path.exists(PROJECT_ROOT_LOCAL):\n",
        "    raise FileNotFoundError(f\"{PROJECT_ROOT_LOCAL} folder not found\")\n",
        "\n",
        "os.chdir(PROJECT_ROOT_LOCAL)\n",
        "\n",
        "if PROJECT_ROOT_LOCAL not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT_LOCAL)\n",
        "\n",
        "print(f\"✅ Setup completed!\")\n",
        "print(f\"   Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2b29dcf9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b29dcf9",
        "outputId": "755c310d-500f-43d8-c034-84cfdbbfb395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.10.1\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from src import config, models\n",
        "from src.models import VAE\n",
        "\n",
        "# Verifica GPU\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527ee2da",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_PATH = \"data/synthetic_dataset.npz\"\n",
        "MODELS_FOLDER = \"models\"\n",
        "MODELS_WEIGHTS_PATH = \"models/weights/beta_vae_weights.h5\"\n",
        "\n",
        "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
        "\n",
        "INPUT_SHAPE = (16, 512, 1)\n",
        "LATENT_DIM = 8\n",
        "ALPHA = 10.0\n",
        "BETA = 1.5\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6822f96d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tf_dataset(recon_data, true_data, false_data, batch_size):\n",
        "    \"\"\"\n",
        "    Crea un tf.data.Dataset ottimizzato a partire dai tensori numpy\n",
        "    \"\"\"\n",
        "    # Creazione del dataset da slice di tensori\n",
        "    # Input X: (Reconstruction, True Cadence, False Cadence)\n",
        "    # Target Y: Reconstruction\n",
        "    ds = tf.data.Dataset.from_tensor_slices(\n",
        "        ((recon_data, true_data, false_data), recon_data)\n",
        "    )\n",
        "    \n",
        "    # Shuffle, Batching e Prefetching\n",
        "    # drop_remainder=True è importante per training multi-GPU (MirroredStrategy)\n",
        "    ds = ds.shuffle(2048).batch(batch_size, drop_remainder=True)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return ds\n",
        "\n",
        "def load_data(filepath, split=0.8):\n",
        "    \"\"\"\n",
        "    Carica i dati dal file .npz, esegue lo split e restituisce i dataset TF pronti.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"File {filepath} non trovato!\")\n",
        "        \n",
        "    print(f\"Loading data from {filepath}...\")\n",
        "    with np.load(filepath) as f:\n",
        "        mixed = f['mixed_cadence']\n",
        "        true_c = f['true_cadence']\n",
        "        false_c = f['false_cadence']\n",
        "        \n",
        "    print(f\"Total samples: {len(mixed)}\")\n",
        "    \n",
        "    # Prepariamo l'input per la ricostruzione (Solo il primo frame ON della sequenza)\n",
        "    # Assumiamo che mixed sia (N, 6, 16, 512, 1) -> x_recon diventa (N, 16, 512, 1)\n",
        "    x_recon = mixed[:, 0, :, :, :] \n",
        "    \n",
        "    # Creazione degli indici per lo split\n",
        "    indices = np.arange(len(mixed))\n",
        "    train_idx, val_idx = train_test_split(indices, train_size=split, random_state=42)\n",
        "        \n",
        "    # Creazione Train Dataset\n",
        "    # Passiamo direttamente le slice degli array usando gli indici di train\n",
        "    train_ds = create_tf_dataset(\n",
        "        recon_data=x_recon[train_idx],\n",
        "        true_data=true_c[train_idx],\n",
        "        false_data=false_c[train_idx],\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "    \n",
        "    # Creazione Validation Dataset\n",
        "    # Passiamo direttamente le slice degli array usando gli indici di val\n",
        "    val_ds = create_tf_dataset(\n",
        "        recon_data=x_recon[val_idx],\n",
        "        true_data=true_c[val_idx],\n",
        "        false_data=false_c[val_idx],\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "    \n",
        "    return train_ds, val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "da0b5e25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from data/synthetic_dataset.npz...\n",
            "Total samples: 128\n"
          ]
        }
      ],
      "source": [
        "train_dataset, val_dataset = load_data(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61e6897",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "tensorflow      WARNING  There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "tensorflow      INFO     Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ]
        }
      ],
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope(): \n",
        "    encoder = models.build_encoder(latent_dim=LATENT_DIM)\n",
        "    decoder = models.build_decoder(latent_dim=LATENT_DIM)\n",
        "    vae = VAE(encoder, decoder, alpha=ALPHA, beta=BETA)\n",
        "\n",
        "    # Compilazione con ottimizzatore standard\n",
        "    vae.compile(optimizer=keras.optimizers.Adam(learning_rate=LR))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "34ae6d11",
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_total_loss\", \n",
        "        patience=10, \n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_total_loss\",\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        verbose=1\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "57745bf7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 16:17:16.648890: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_DOUBLE\n",
            "      type: DT_DOUBLE\n",
            "      type: DT_DOUBLE\n",
            "      type: DT_DOUBLE\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 102\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:8\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: 16\n",
            "        }\n",
            "        dim {\n",
            "          size: 512\n",
            "        }\n",
            "        dim {\n",
            "          size: 1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: 6\n",
            "        }\n",
            "        dim {\n",
            "          size: 16\n",
            "        }\n",
            "        dim {\n",
            "          size: 512\n",
            "        }\n",
            "        dim {\n",
            "          size: 1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: 6\n",
            "        }\n",
            "        dim {\n",
            "          size: 16\n",
            "        }\n",
            "        dim {\n",
            "          size: 512\n",
            "        }\n",
            "        dim {\n",
            "          size: 1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: 16\n",
            "        }\n",
            "        dim {\n",
            "          size: 512\n",
            "        }\n",
            "        dim {\n",
            "          size: 1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"replicate_on_split\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "experimental_type {\n",
            "  type_id: TFT_PRODUCT\n",
            "  args {\n",
            "    type_id: TFT_DATASET\n",
            "    args {\n",
            "      type_id: TFT_PRODUCT\n",
            "      args {\n",
            "        type_id: TFT_TENSOR\n",
            "        args {\n",
            "          type_id: TFT_DOUBLE\n",
            "        }\n",
            "      }\n",
            "      args {\n",
            "        type_id: TFT_TENSOR\n",
            "        args {\n",
            "          type_id: TFT_DOUBLE\n",
            "        }\n",
            "      }\n",
            "      args {\n",
            "        type_id: TFT_TENSOR\n",
            "        args {\n",
            "          type_id: TFT_DOUBLE\n",
            "        }\n",
            "      }\n",
            "      args {\n",
            "        type_id: TFT_TENSOR\n",
            "        args {\n",
            "          type_id: TFT_DOUBLE\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "2025-11-24 16:17:16.791790: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/keras/engine/training.py:1576\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1574\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty logs). Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1583\u001b[0m     )\n\u001b[1;32m   1584\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(logs)\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;66;03m# Run validation.\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ],
      "source": [
        "print(\"Starting training...\")\n",
        "\n",
        "history = vae.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_gbt_seti",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
