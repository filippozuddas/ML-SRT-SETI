{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filippozuddas/ML-SRT-SETI/blob/main/notebooks/train_CVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "TPoHtJa2I_E4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TPoHtJa2I_E4",
        "outputId": "22c096c8-c7a3-4812-ec07-a68be7dfc702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting setigen\n",
            "  Downloading setigen-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from setigen) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from setigen) (1.15.3)\n",
            "Requirement already satisfied: astropy>=5.0.4 in /usr/local/lib/python3.11/dist-packages (from setigen) (7.1.0)\n",
            "Collecting blimpy>=2.1.4 (from setigen)\n",
            "  Downloading blimpy-2.1.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from setigen) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.11/dist-packages (from setigen) (4.67.1)\n",
            "Collecting sphinx-rtd-theme>=0.4.3 (from setigen)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting pytest-cov>=4.1.0 (from setigen)\n",
            "  Downloading pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.4.28.0.37.27 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (0.2025.6.23.0.39.50)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (6.0.2)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy>=5.0.4->setigen) (24.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (75.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (3.14.0)\n",
            "Collecting hdf5plugin (from blimpy>=2.1.4->setigen)\n",
            "  Downloading hdf5plugin-6.0.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from blimpy>=2.1.4->setigen) (5.9.5)\n",
            "Collecting pyparsing==2.4.7 (from blimpy>=2.1.4->setigen)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (1.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->setigen) (2.9.0.post0)\n",
            "Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov>=4.1.0->setigen)\n",
            "  Downloading coverage-7.11.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: pluggy>=1.2 in /usr/local/lib/python3.11/dist-packages (from pytest-cov>=4.1.0->setigen) (1.6.0)\n",
            "Requirement already satisfied: pytest>=7 in /usr/local/lib/python3.11/dist-packages (from pytest-cov>=4.1.0->setigen) (8.3.5)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme>=0.4.3->setigen) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme>=0.4.3->setigen) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme>=0.4.3->setigen)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=7->pytest-cov>=4.1.0->setigen) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->blimpy>=2.1.4->setigen) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->blimpy>=2.1.4->setigen) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx<9,>=6->sphinx-rtd-theme>=0.4.3->setigen) (2025.6.15)\n",
            "Downloading setigen-2.7.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blimpy-2.1.4-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_cov-7.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coverage-7.11.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (249 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m249.3/249.3 kB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdf5plugin-6.0.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (44.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyparsing, coverage, hdf5plugin, sphinxcontrib-jquery, pytest-cov, blimpy, sphinx-rtd-theme, setigen\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydot 3.0.4 requires pyparsing>=3.0.9, but you have pyparsing 2.4.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blimpy-2.1.4 coverage-7.11.3 hdf5plugin-6.0.0 pyparsing-2.4.7 pytest-cov-7.0.0 setigen-2.7.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "14f022560c0f40b39d44ca6f845cc958",
              "pip_warning": {
                "packages": [
                  "pyparsing",
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install setigen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "vVtKbNZ0Jc_9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVtKbNZ0Jc_9",
        "outputId": "11117107-0efa-4c86-c72d-73af2af94505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Setup completed!\n",
            "   Working directory: /home/filippo/TirocinioSETI/ML-SRT-SETI\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# --- CONFIGURAZIONE PERCORSI ---\n",
        "PROJECT_ROOT = \"/content/filippo/ML-SRT-SETI\"\n",
        "PROJECT_ROOT_LOCAL = \"/home/filippo/TirocinioSETI/ML-SRT-SETI\"\n",
        "\n",
        "if not os.path.exists(PROJECT_ROOT_LOCAL):\n",
        "    raise FileNotFoundError(f\"{PROJECT_ROOT_LOCAL} folder not found\")\n",
        "\n",
        "os.chdir(PROJECT_ROOT_LOCAL)\n",
        "\n",
        "if PROJECT_ROOT_LOCAL not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT_LOCAL)\n",
        "\n",
        "print(f\"âœ… Setup completed!\")\n",
        "print(f\"   Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2b29dcf9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b29dcf9",
        "outputId": "755c310d-500f-43d8-c034-84cfdbbfb395"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 17:14:16.106203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-17 17:14:16.650519: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 17:14:16.724509: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2025-11-17 17:14:16.724620: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-11-17 17:14:16.825603: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-11-17 17:14:18.836140: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2025-11-17 17:14:18.836466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2025-11-17 17:14:18.836485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/home/filippo/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/blimpy/__init__.py:21: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import get_distribution, DistributionNotFound\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.10.1\n",
            "Num GPUs Available:  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 17:14:23.898686: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2025-11-17 17:14:23.899283: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (filippo-ideapad-slim-5): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from src import config, models\n",
        "\n",
        "# Verifica GPU\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b720072",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b720072",
        "outputId": "a9b489bc-1036-453e-c244-d3db7b9cf4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caricamento dati da data/processed/train_class0_5k.npy...\n"
          ]
        }
      ],
      "source": [
        "# Percorso del file generato nello step precedente\n",
        "DATA_PATH = \"data/processed/train_class0_5k.npy\"\n",
        "\n",
        "# 1. Carica i dati\n",
        "print(f\"Caricamento dati da {DATA_PATH}...\")\n",
        "dataset_cadences = np.load(DATA_PATH)\n",
        "print(f\"Shape originale (Cadenze): {dataset_cadences.shape}\")\n",
        "\n",
        "# 2. Reshape: Da (N_samples, 6, 16, 512, 1) a (N_samples * 6, 16, 512, 1)\n",
        "# Fondiamo la dimensione delle cadenze con quella dei campioni\n",
        "# -1 dice a numpy di calcolare automaticamente la dimensione\n",
        "X_all = dataset_cadences.reshape(-1, config.TCHANS, config.FCHANS_FINAL, 1)\n",
        "\n",
        "print(f\"Shape per il training (Frames singoli): {X_all.shape}\")\n",
        "\n",
        "# 3. Train/Test Split\n",
        "# Usiamo il random_state per riproducibilitÃ \n",
        "X_train, X_val = train_test_split(X_all, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training frames: {X_train.shape}\")\n",
        "print(f\"Validation frames: {X_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "RxTmi8-vQnJ1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxTmi8-vQnJ1",
        "outputId": "ffbd57ff-697f-450d-e92a-5133385d2540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifica integritÃ  dati...\n",
            "Min value: 0.0\n",
            "Max value: 1.0\n",
            "Contains NaNs: False\n",
            "Contains Infs: False\n",
            "âœ… Dati puliti. Il problema Ã¨ nell'architettura o nei parametri.\n"
          ]
        }
      ],
      "source": [
        "print(\"Verifica integritÃ  dati...\")\n",
        "print(f\"Min value: {np.min(X_train)}\")\n",
        "print(f\"Max value: {np.max(X_train)}\")\n",
        "print(f\"Contains NaNs: {np.isnan(X_train).any()}\")\n",
        "print(f\"Contains Infs: {np.isinf(X_train).any()}\")\n",
        "\n",
        "if np.isnan(X_train).any() or np.isinf(X_train).any():\n",
        "    print(\"âš ï¸ ATTENZIONE: Il dataset contiene valori non validi! Il problema Ã¨ nel preprocessing.\")\n",
        "else:\n",
        "    print(\"âœ… Dati puliti. Il problema Ã¨ nell'architettura o nei parametri.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d71b82aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "d71b82aa",
        "outputId": "92740cea-ef47-4e9d-ab44-e33fd467265b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MirroredStrategy copia il modello su tutte le GPU disponibile e divide il batch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m strategy \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mMirroredStrategy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Parametri\u001b[39;00m\n\u001b[1;32m      5\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (config\u001b[38;5;241m.\u001b[39mTCHANS, config\u001b[38;5;241m.\u001b[39mFCHANS_FINAL, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "# MirroredStrategy copia il modello su tutte le GPU disponibile e divide il batch\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# Parametri\n",
        "input_shape = (config.TCHANS, config.FCHANS_FINAL, 1)\n",
        "latent_dim = config.LATENT_DIM # 8\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "  # Costruzione Encoder e Decoder\n",
        "  encoder = models.build_encoder(input_shape, latent_dim)\n",
        "  decoder = models.build_decoder(latent_dim, target_shape=input_shape)\n",
        "\n",
        "  # 2. CVAE Completo\n",
        "  # beta=1.0 Ã¨ un VAE standard.\n",
        "  # beta > 1 forza una rappresentazione latente piÃ¹ \"districata\" (disentangled)\n",
        "  cvae = models.CVAE(encoder, decoder, beta=1)\n",
        "\n",
        "  # Compile\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  cvae.compile(optimizer=optimizer)\n",
        "\n",
        "# (Opzionale) Visualizza sommario encoder\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4281e14c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "4281e14c",
        "outputId": "4b9250bc-c3f0-48a1-a6df-6a7e065a532a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avvio training...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 15:33:20.238495: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "750/750 [==============================] - ETA: 0s - total_loss: 4391.9995 - reconstruction_loss: 4377.3174 - kl_loss: 14.6829"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 15:37:39.546036: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "750/750 [==============================] - 284s 374ms/step - total_loss: 4391.9995 - reconstruction_loss: 4377.3174 - kl_loss: 14.6829 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_val_total_loss: 4281.4121 - val_val_reconstruction_loss: 4272.2764 - val_val_kl_loss: 9.1371\n",
            "Epoch 2/20\n",
            "699/750 [==========================>...] - ETA: 16s - total_loss: 4270.0732 - reconstruction_loss: 4261.9990 - kl_loss: 8.0724"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mBATCH_SIZE \u001b[38;5;66;03m# 32\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvvio training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Nel VAE input e target sono uguali (ricostruzione)\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completato.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m~/miniconda3/envs/ml_gbt_seti/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Configurazione Training\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = config.BATCH_SIZE # 32\n",
        "\n",
        "print(\"Avvio training...\")\n",
        "\n",
        "history = cvae.fit(\n",
        "    X_train,\n",
        "    validation_data=(X_val, X_val), # Nel VAE input e target sono uguali (ricostruzione)\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Training completato.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61e6897",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from src.generator import SetigenGenerator\n",
        "from src.modelsv2 import SETI_VAE\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "# Hardware & Percorsi\n",
        "GPUS = tf.config.list_physical_devices('GPU')\n",
        "LOG_DIR = \"logs\"\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Iperparametri\n",
        "# Batch size globale: con 2x4090 possiamo spingere.\n",
        "# Inizia con 64 o 128. Se hai OOM (Out of Memory), riduci.\n",
        "BATCH_SIZE_PER_REPLICA = 64 \n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * len(GPUS) if len(GPUS) > 0 else BATCH_SIZE_PER_REPLICA\n",
        "\n",
        "EPOCHS = 100\n",
        "STEPS_PER_EPOCH = 1000  # Quanti batch processare per considerare finita un'epoca\n",
        "LEARNING_RATE = 1e-4    # Tasso di apprendimento iniziale (Paper: 0.0005)\n",
        "LATENT_DIM = 8          # Dimensione spazio latente (Paper: 8)\n",
        "\n",
        "# Parametri Generatore (Simulazione)\n",
        "SNR_RANGE = (10, 50)\n",
        "DRIFT_RANGE = (0.5, 8.0)\n",
        "\n",
        "def main():\n",
        "    print(f\"ðŸš€ Avvio Training SETI VAE\")\n",
        "    print(f\"Num GPUs Available: {len(GPUS)}\")\n",
        "    print(f\"Global Batch Size: {GLOBAL_BATCH_SIZE}\")\n",
        "    \n",
        "    # 1. Setup Strategia Multi-GPU\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f'Number of devices: {strategy.num_replicas_in_sync}')\n",
        "\n",
        "    # 2. Preparazione Dati\n",
        "    # Creiamo due generatori: uno per training e uno per validation\n",
        "    # (Nota: essendo dati sintetici, la validation serve a controllare che la loss scenda su dati nuovi)\n",
        "    \n",
        "    train_gen = SetigenGenerator(\n",
        "        batch_size=GLOBAL_BATCH_SIZE,\n",
        "        snr_range=SNR_RANGE,\n",
        "        drift_range=DRIFT_RANGE\n",
        "    )\n",
        "    train_dataset = train_gen.get_dataset()\n",
        "\n",
        "    # Dataset di Validazione (Separiamo per monitorare l'overfitting)\n",
        "    val_gen = SetigenGenerator(\n",
        "        batch_size=GLOBAL_BATCH_SIZE,\n",
        "        snr_range=SNR_RANGE, \n",
        "        drift_range=DRIFT_RANGE\n",
        "    )\n",
        "    val_dataset = val_gen.get_dataset()\n",
        "\n",
        "    # 3. Costruzione Modello nello Scope della Strategia\n",
        "    with strategy.scope():\n",
        "        # Istanziamo il modello VAE\n",
        "        # Input shape: (6, 16, 512, 1) - La cadenza intera\n",
        "        vae = SETI_VAE(\n",
        "            input_shape=(6, 16, 512, 1),\n",
        "            latent_dim=LATENT_DIM,\n",
        "            beta=1.5,  # Peso KL\n",
        "            alpha=10.0 # Peso Clustering\n",
        "        )\n",
        "        \n",
        "        # Compiliamo il modello\n",
        "        # Non passiamo una loss function qui perchÃ© Ã¨ gestita internamente nel train_step del modello custom\n",
        "        vae.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
        "\n",
        "    # 4. Callbacks\n",
        "    # Nome univoco per il run\n",
        "    run_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    \n",
        "    # Salva i pesi migliori\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"vae_seti_{run_name}_best.h5\")\n",
        "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True, # Salviamo solo i pesi per flessibilitÃ  (custom model)\n",
        "        monitor='val_total_loss', # Monitoriamo la loss totale validazione\n",
        "        mode='min',\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Log CSV per grafici\n",
        "    csv_logger = keras.callbacks.CSVLogger(os.path.join(LOG_DIR, f\"training_log_{run_name}.csv\"))\n",
        "    \n",
        "    # Riduzione LR se ci blocchiamo\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_total_loss', \n",
        "        factor=0.5, \n",
        "        patience=5, \n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Early Stopping\n",
        "    early_stop = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_total_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 5. Avvio Training\n",
        "    print(\"\\nDataset e Modello pronti. Inizio fit()...\")\n",
        "    \n",
        "    history = vae.fit(\n",
        "        train_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        steps_per_epoch=STEPS_PER_EPOCH,\n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=100, # 100 batch di validazione\n",
        "        callbacks=[checkpoint_cb, csv_logger, reduce_lr, early_stop]\n",
        "    )\n",
        "\n",
        "    print(\"âœ… Training completato.\")\n",
        "    \n",
        "    # Salvataggio finale (modello completo se possibile, o pesi)\n",
        "    final_save_path = os.path.join(CHECKPOINT_DIR, f\"vae_seti_{run_name}_final_weights.h5\")\n",
        "    vae.save_weights(final_save_path)\n",
        "    print(f\"Pesi finali salvati in: {final_save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_gbt_seti",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
